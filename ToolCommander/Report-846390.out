---------------------------------------
Begin Slurm Prolog: Sep-11-2025 14:24:50
Job ID:    846390
User ID:   ychauhan9
Account:   gts-tbaluta3
Job name:  ToolEvaluation
Partition: gpu-l40s
QOS:       embers
---------------------------------------
Thu Sep 11 14:24:52 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    On  |   00000000:C1:00.0 Off |                    0 |
| N/A   30C    P8             31W /  350W |       0MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA L40S                    On  |   00000000:C4:00.0 Off |                    0 |
| N/A   31C    P8             34W /  350W |       0MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Evaluating LLM: localg, Retriever: facebook/contriever-msmarco, Result File: ./attack_results/g1_train_a/attack_results_facebook_contriever-msmarco.json, B-Tool: tool1...
If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`
Some weights of BertLMHeadModel were not initialized from the model checkpoint at facebook/contriever-msmarco and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating LLM: localg, with Retriever: facebook/contriever-msmarco, using B-Tool: tool1
Using attacked results from: ./attack_results/g1_train_a/attack_results_facebook_contriever-msmarco.json
Training data: ./data/g1_train_a.json, Evaluation data: ./data/g1_eval_a.json
Evaluating stage1...
Building corpus...
Building corpus embeddings...
  0%|          | 0/41 [00:00<?, ?it/s]  2%|▏         | 1/41 [00:00<00:18,  2.17it/s]  7%|▋         | 3/41 [00:00<00:08,  4.63it/s] 10%|▉         | 4/41 [00:00<00:07,  4.76it/s] 12%|█▏        | 5/41 [00:01<00:07,  4.85it/s] 15%|█▍        | 6/41 [00:01<00:07,  4.91it/s] 17%|█▋        | 7/41 [00:01<00:06,  4.95it/s] 20%|█▉        | 8/41 [00:01<00:06,  4.97it/s] 22%|██▏       | 9/41 [00:01<00:06,  4.99it/s] 24%|██▍       | 10/41 [00:02<00:06,  5.00it/s] 27%|██▋       | 11/41 [00:02<00:05,  5.01it/s] 29%|██▉       | 12/41 [00:02<00:05,  5.02it/s] 32%|███▏      | 13/41 [00:02<00:05,  5.03it/s] 34%|███▍      | 14/41 [00:02<00:05,  5.03it/s] 37%|███▋      | 15/41 [00:03<00:05,  5.04it/s] 39%|███▉      | 16/41 [00:03<00:04,  5.03it/s] 41%|████▏     | 17/41 [00:03<00:04,  5.04it/s] 44%|████▍     | 18/41 [00:03<00:04,  5.03it/s] 46%|████▋     | 19/41 [00:03<00:04,  5.04it/s] 49%|████▉     | 20/41 [00:04<00:04,  5.04it/s] 51%|█████     | 21/41 [00:04<00:03,  5.04it/s] 54%|█████▎    | 22/41 [00:04<00:03,  5.04it/s] 56%|█████▌    | 23/41 [00:04<00:03,  5.03it/s] 59%|█████▊    | 24/41 [00:04<00:03,  5.02it/s] 61%|██████    | 25/41 [00:05<00:03,  5.02it/s] 63%|██████▎   | 26/41 [00:05<00:02,  5.02it/s] 66%|██████▌   | 27/41 [00:05<00:02,  5.02it/s] 68%|██████▊   | 28/41 [00:05<00:02,  5.03it/s] 71%|███████   | 29/41 [00:05<00:02,  5.03it/s] 73%|███████▎  | 30/41 [00:06<00:02,  5.03it/s] 76%|███████▌  | 31/41 [00:06<00:01,  5.03it/s] 78%|███████▊  | 32/41 [00:06<00:01,  5.02it/s] 80%|████████  | 33/41 [00:06<00:01,  5.03it/s] 83%|████████▎ | 34/41 [00:06<00:01,  5.03it/s] 85%|████████▌ | 35/41 [00:07<00:01,  5.03it/s] 88%|████████▊ | 36/41 [00:07<00:00,  5.02it/s] 90%|█████████ | 37/41 [00:07<00:00,  5.03it/s] 93%|█████████▎| 38/41 [00:07<00:00,  5.02it/s] 95%|█████████▌| 39/41 [00:07<00:00,  5.03it/s] 98%|█████████▊| 40/41 [00:08<00:00,  5.03it/s]100%|██████████| 41/41 [00:08<00:00,  5.04it/s]100%|██████████| 41/41 [00:08<00:00,  4.96it/s]
MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.49s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:06<00:03,  3.27s/it]
Traceback (most recent call last):
  File "/storage/scratch1/6/ychauhan9/ToolCommander/evaluate.py", line 615, in <module>
    fire.Fire(main)
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/scratch1/6/ychauhan9/ToolCommander/evaluate.py", line 579, in main
    eval(
  File "/storage/scratch1/6/ychauhan9/ToolCommander/evaluate.py", line 518, in eval
    LLM_MODEL, LLM_TOKENIZER = load_model(
                               ^^^^^^^^^^^
  File "/storage/scratch1/6/ychauhan9/ToolCommander/utils.py", line 163, in load_model
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/modeling_utils.py", line 317, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5069, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5532, in _load_pretrained_model
    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)
                                                         ^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/modeling_utils.py", line 975, in load_shard_file
    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/modeling_utils.py", line 883, in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/quantizers/quantizer_mxfp4.py", line 247, in create_quantized_param
    dequantize(module, param_name, param_value, target_device, dq_param_name, **shard_kwargs)
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/integrations/mxfp4.py", line 342, in dequantize
    dequantized = convert_moe_packed_tensors(getattr(module, blocks_attr), getattr(module, scales_attr))
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/integrations/mxfp4.py", line 124, in convert_moe_packed_tensors
    idx_hi = (blk >> 4).to(torch.long)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 44.39 GiB of which 881.31 MiB is free. Including non-PyTorch memory, this process has 43.52 GiB memory in use. Of the allocated memory 41.27 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
srun: error: atl1-1-01-010-29-0: task 0: Exited with exit code 1
Evaluating LLM: localg, Retriever: facebook/contriever-msmarco, Result File: ./attack_results/g1_train_b/attack_results_facebook_contriever-msmarco.json, B-Tool: tool2...
If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`
Some weights of BertLMHeadModel were not initialized from the model checkpoint at facebook/contriever-msmarco and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating LLM: localg, with Retriever: facebook/contriever-msmarco, using B-Tool: tool2
Using attacked results from: ./attack_results/g1_train_b/attack_results_facebook_contriever-msmarco.json
Training data: ./data/g1_train_a.json, Evaluation data: ./data/g1_eval_a.json
Evaluating stage1...
Building corpus...
Building corpus embeddings...
  0%|          | 0/41 [00:00<?, ?it/s]  2%|▏         | 1/41 [00:00<00:18,  2.17it/s]  7%|▋         | 3/41 [00:00<00:08,  4.65it/s] 10%|▉         | 4/41 [00:00<00:07,  4.77it/s] 12%|█▏        | 5/41 [00:01<00:07,  4.86it/s] 15%|█▍        | 6/41 [00:01<00:07,  4.91it/s] 17%|█▋        | 7/41 [00:01<00:06,  4.95it/s] 20%|█▉        | 8/41 [00:01<00:06,  4.97it/s] 22%|██▏       | 9/41 [00:01<00:06,  5.00it/s] 24%|██▍       | 10/41 [00:02<00:06,  5.01it/s] 27%|██▋       | 11/41 [00:02<00:05,  5.01it/s] 29%|██▉       | 12/41 [00:02<00:05,  5.02it/s] 32%|███▏      | 13/41 [00:02<00:05,  5.02it/s] 34%|███▍      | 14/41 [00:02<00:05,  5.03it/s] 37%|███▋      | 15/41 [00:03<00:05,  5.04it/s] 39%|███▉      | 16/41 [00:03<00:04,  5.04it/s] 41%|████▏     | 17/41 [00:03<00:04,  5.04it/s] 44%|████▍     | 18/41 [00:03<00:04,  5.03it/s] 46%|████▋     | 19/41 [00:03<00:04,  5.04it/s] 49%|████▉     | 20/41 [00:04<00:04,  5.04it/s] 51%|█████     | 21/41 [00:04<00:03,  5.04it/s] 54%|█████▎    | 22/41 [00:04<00:03,  5.04it/s] 56%|█████▌    | 23/41 [00:04<00:03,  5.03it/s] 59%|█████▊    | 24/41 [00:04<00:03,  5.02it/s] 61%|██████    | 25/41 [00:05<00:03,  5.02it/s] 63%|██████▎   | 26/41 [00:05<00:02,  5.02it/s] 66%|██████▌   | 27/41 [00:05<00:02,  5.02it/s] 68%|██████▊   | 28/41 [00:05<00:02,  5.02it/s] 71%|███████   | 29/41 [00:05<00:02,  5.02it/s] 73%|███████▎  | 30/41 [00:06<00:02,  5.03it/s] 76%|███████▌  | 31/41 [00:06<00:01,  5.03it/s] 78%|███████▊  | 32/41 [00:06<00:01,  5.03it/s] 80%|████████  | 33/41 [00:06<00:01,  5.04it/s] 83%|████████▎ | 34/41 [00:06<00:01,  5.03it/s] 85%|████████▌ | 35/41 [00:07<00:01,  5.04it/s] 88%|████████▊ | 36/41 [00:07<00:00,  5.04it/s] 90%|█████████ | 37/41 [00:07<00:00,  5.04it/s] 93%|█████████▎| 38/41 [00:07<00:00,  5.03it/s] 95%|█████████▌| 39/41 [00:07<00:00,  5.04it/s] 98%|█████████▊| 40/41 [00:08<00:00,  5.04it/s]100%|██████████| 41/41 [00:08<00:00,  5.05it/s]100%|██████████| 41/41 [00:08<00:00,  4.96it/s]
MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.49s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.48s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:06<00:03,  3.26s/it]
Traceback (most recent call last):
  File "/storage/scratch1/6/ychauhan9/ToolCommander/evaluate.py", line 615, in <module>
    fire.Fire(main)
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/scratch1/6/ychauhan9/ToolCommander/evaluate.py", line 579, in main
    eval(
  File "/storage/scratch1/6/ychauhan9/ToolCommander/evaluate.py", line 518, in eval
    LLM_MODEL, LLM_TOKENIZER = load_model(
                               ^^^^^^^^^^^
  File "/storage/scratch1/6/ychauhan9/ToolCommander/utils.py", line 163, in load_model
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/modeling_utils.py", line 317, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5069, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5532, in _load_pretrained_model
    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)
                                                         ^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/modeling_utils.py", line 975, in load_shard_file
    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/modeling_utils.py", line 883, in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/quantizers/quantizer_mxfp4.py", line 247, in create_quantized_param
    dequantize(module, param_name, param_value, target_device, dq_param_name, **shard_kwargs)
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/integrations/mxfp4.py", line 342, in dequantize
    dequantized = convert_moe_packed_tensors(getattr(module, blocks_attr), getattr(module, scales_attr))
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/integrations/mxfp4.py", line 124, in convert_moe_packed_tensors
    idx_hi = (blk >> 4).to(torch.long)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 44.39 GiB of which 881.31 MiB is free. Including non-PyTorch memory, this process has 43.52 GiB memory in use. Of the allocated memory 41.27 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
srun: error: atl1-1-01-010-29-0: task 0: Exited with exit code 1
Evaluating LLM: localg, Retriever: facebook/contriever-msmarco, Result File: ./attack_results/g1_train_c/attack_results_facebook_contriever-msmarco.json, B-Tool: tool3...
If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`
Some weights of BertLMHeadModel were not initialized from the model checkpoint at facebook/contriever-msmarco and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating LLM: localg, with Retriever: facebook/contriever-msmarco, using B-Tool: tool3
Using attacked results from: ./attack_results/g1_train_c/attack_results_facebook_contriever-msmarco.json
Training data: ./data/g1_train_a.json, Evaluation data: ./data/g1_eval_a.json
Evaluating stage1...
Building corpus...
Building corpus embeddings...
  0%|          | 0/41 [00:00<?, ?it/s]  2%|▏         | 1/41 [00:00<00:18,  2.18it/s]  7%|▋         | 3/41 [00:00<00:08,  4.67it/s] 10%|▉         | 4/41 [00:00<00:07,  4.78it/s] 12%|█▏        | 5/41 [00:01<00:07,  4.87it/s] 15%|█▍        | 6/41 [00:01<00:07,  4.92it/s] 17%|█▋        | 7/41 [00:01<00:06,  4.95it/s] 20%|█▉        | 8/41 [00:01<00:06,  4.97it/s] 22%|██▏       | 9/41 [00:01<00:06,  4.98it/s] 24%|██▍       | 10/41 [00:02<00:06,  4.99it/s] 27%|██▋       | 11/41 [00:02<00:05,  5.00it/s] 29%|██▉       | 12/41 [00:02<00:05,  5.02it/s] 32%|███▏      | 13/41 [00:02<00:05,  5.03it/s] 34%|███▍      | 14/41 [00:02<00:05,  5.03it/s] 37%|███▋      | 15/41 [00:03<00:05,  5.04it/s] 39%|███▉      | 16/41 [00:03<00:04,  5.04it/s] 41%|████▏     | 17/41 [00:03<00:04,  5.04it/s] 44%|████▍     | 18/41 [00:03<00:04,  5.04it/s] 46%|████▋     | 19/41 [00:03<00:04,  5.04it/s] 49%|████▉     | 20/41 [00:04<00:04,  5.04it/s] 51%|█████     | 21/41 [00:04<00:03,  5.05it/s] 54%|█████▎    | 22/41 [00:04<00:03,  5.04it/s] 56%|█████▌    | 23/41 [00:04<00:03,  5.03it/s] 59%|█████▊    | 24/41 [00:04<00:03,  5.03it/s] 61%|██████    | 25/41 [00:05<00:03,  5.02it/s] 63%|██████▎   | 26/41 [00:05<00:02,  5.02it/s] 66%|██████▌   | 27/41 [00:05<00:02,  5.02it/s] 68%|██████▊   | 28/41 [00:05<00:02,  5.02it/s] 71%|███████   | 29/41 [00:05<00:02,  5.02it/s] 73%|███████▎  | 30/41 [00:06<00:02,  5.02it/s] 76%|███████▌  | 31/41 [00:06<00:01,  5.01it/s] 78%|███████▊  | 32/41 [00:06<00:01,  5.01it/s] 80%|████████  | 33/41 [00:06<00:01,  5.02it/s] 83%|████████▎ | 34/41 [00:06<00:01,  5.01it/s] 85%|████████▌ | 35/41 [00:07<00:01,  5.01it/s] 88%|████████▊ | 36/41 [00:07<00:00,  5.01it/s] 90%|█████████ | 37/41 [00:07<00:00,  5.02it/s] 93%|█████████▎| 38/41 [00:07<00:00,  5.02it/s] 95%|█████████▌| 39/41 [00:07<00:00,  5.02it/s] 98%|█████████▊| 40/41 [00:08<00:00,  5.02it/s]100%|██████████| 41/41 [00:08<00:00,  5.03it/s]100%|██████████| 41/41 [00:08<00:00,  4.95it/s]
MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.46s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.45s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:06<00:03,  3.23s/it]
Traceback (most recent call last):
  File "/storage/scratch1/6/ychauhan9/ToolCommander/evaluate.py", line 615, in <module>
    fire.Fire(main)
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/scratch1/6/ychauhan9/ToolCommander/evaluate.py", line 579, in main
    eval(
  File "/storage/scratch1/6/ychauhan9/ToolCommander/evaluate.py", line 518, in eval
    LLM_MODEL, LLM_TOKENIZER = load_model(
                               ^^^^^^^^^^^
  File "/storage/scratch1/6/ychauhan9/ToolCommander/utils.py", line 163, in load_model
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/modeling_utils.py", line 317, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5069, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5532, in _load_pretrained_model
    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)
                                                         ^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/modeling_utils.py", line 975, in load_shard_file
    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/modeling_utils.py", line 883, in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/quantizers/quantizer_mxfp4.py", line 247, in create_quantized_param
    dequantize(module, param_name, param_value, target_device, dq_param_name, **shard_kwargs)
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/integrations/mxfp4.py", line 342, in dequantize
    dequantized = convert_moe_packed_tensors(getattr(module, blocks_attr), getattr(module, scales_attr))
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/6/ychauhan9/.conda/envs/toolcommander/lib/python3.11/site-packages/transformers/integrations/mxfp4.py", line 124, in convert_moe_packed_tensors
    idx_hi = (blk >> 4).to(torch.long)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 44.39 GiB of which 881.31 MiB is free. Including non-PyTorch memory, this process has 43.52 GiB memory in use. Of the allocated memory 41.27 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
srun: error: atl1-1-01-010-29-0: task 0: Exited with exit code 1
All evaluations completed!
---------------------------------------
Begin Slurm Epilog: Sep-11-2025 14:26:35
Job ID:        846390
User ID:       ychauhan9
Account:       gts-tbaluta3
Job name:      ToolEvaluation
Resources:     cpu=8,gres/gpu:l40s=2,mem=96G,node=1
Rsrc Used:     cput=00:14:08,vmem=0,walltime=00:01:46,mem=38812K,energy_used=0
Partition:     gpu-l40s
QOS:           embers
Nodes:         atl1-1-01-010-29-0
---------------------------------------
